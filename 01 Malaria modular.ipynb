{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMqhLMa2ETNLwmrMM9BCwYe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Martinmbiro/Malaria/blob/main/01%20Malaria%20modular.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementing Modules**\n",
        "> In this notebook, I'll be creating modules for the end to end project on cell classification\n",
        "\n",
        "> ðŸ’Ž **Pro Tip**\n",
        "+ Modules help organize code logically, promote code reusability and cleaner code"
      ],
      "metadata": {
        "id": "fRdKbY_Vqhdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to delete helper_modules directory and modules zip folder:\n",
        "# !rm -rf /content/helper_modules\n",
        "# !rm -rf /content/modules.zip"
      ],
      "metadata": {
        "id": "JNSg5A2MLniZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch, torchvision, pathlib\n",
        "import torch, torchvision, pathlib\n",
        "\n",
        "# print versions\n",
        "print(f'torch version: {torch.__version__}')\n",
        "print(f'torchvision version: {torchvision.__version__}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGIUMG1ZrPSM",
        "outputId": "724c8b21-ba9a-4a89-c517-835c50896286"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.6.0+cu124\n",
            "torchvision version: 0.21.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data\n",
        "> First, we'll create a directory to hold all the custom modules we write\n",
        "+ To create directories, we'll make use of the [`pathlib`](https://docs.python.org/3/library/pathlib.html) python module"
      ],
      "metadata": {
        "id": "5vrQKpW2qzba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ“ **Note**  \n",
        "+ To **write** a code cell's content into a `*.py`, file we'll use the _magic command_ `%%writefile filename.py`\n",
        "+ To **append** a code cell's content into a `*.py`, file we'll use the _magic command_ `%%writefile -a filename.py`"
      ],
      "metadata": {
        "id": "LdgmTKCirX8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create directory for helper modules\n",
        "HELPER_MODULES = pathlib.Path('helper_modules')\n",
        "HELPER_MODULES.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "M_5kuaSArg1g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JLKd1dUqHq1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6822d46c-a391-4753-ebad-da1e33f8cf96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing helper_modules/data_loader.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile helper_modules/data_loader.py\n",
        "import torch, kagglehub as kh, shutil, random, zipfile, torchvision, os, numpy as np, shutil\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "import torchvision.transforms.v2 as T\n",
        "from itertools import chain\n",
        "from random import shuffle, sample\n",
        "\n",
        "gen = torch.Generator().manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a helper_modules/data_loader.py\n",
        "\n",
        "# a pytorch Dataset to hold images\n",
        "class CellsDataset(Dataset):\n",
        "  def __init__(self, cells_list:list, transforms:torchvision.transforms.v2.Compose):\n",
        "    self.ls = cells_list\n",
        "    self.transforms = transforms\n",
        "\n",
        "    # get targets\n",
        "    _targets = list()\n",
        "    for x in range(len(self.ls)):\n",
        "      lb = 0 if self.ls[x].parent.name.lower()=='uninfected' else 1\n",
        "      _targets.append(lb)\n",
        "    self.targets = np.array(_targets)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.ls)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = self.transforms(Image.open(self.ls[idx]))\n",
        "    label = self.targets[idx].item()\n",
        "    return image, label\n"
      ],
      "metadata": {
        "id": "dgHnYQw7F8tu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12142a7b-49d7-4abc-b07a-422fedd7981d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to helper_modules/data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a helper_modules/data_loader.py\n",
        "\n",
        "# define image transforms\n",
        "_cell_transform = T.Compose([\n",
        "    T.PILToTensor(),\n",
        "    T.ToDtype(torch.float32, scale=True),\n",
        "    T.Resize((128, 128)),\n",
        "    #imagenet normalization:\n",
        "    #  * can be gotten from calling model.pretrained_cfg property on\n",
        "    #    a timm's pretrained model\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "zoHNHM8AEN9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "552ce9d9-664f-4a60-b23d-7f840c47708c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to helper_modules/data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a helper_modules/data_loader.py\n",
        "\n",
        "# function to download dataset from kaggle, and extract content\n",
        "def _download_dataset():\n",
        "  # create download folder\n",
        "  DOWNLOAD_DIR = Path('malaria')\n",
        "  DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  # download dataset\n",
        "  cache = kh.dataset_download(handle='iarunava/cell-images-for-detecting-malaria')\n",
        "\n",
        "  # archive cache\n",
        "  shutil.make_archive(base_name=DOWNLOAD_DIR/'malaria', format='zip', root_dir=cache)\n",
        "\n",
        "  # extract zipped file\n",
        "  with zipfile.ZipFile(file=DOWNLOAD_DIR/'malaria.zip', mode='r') as zipf:\n",
        "    zipf.extractall(path=DOWNLOAD_DIR)\n",
        "\n",
        "  # delete duplicate sub-directories\n",
        "  if Path.cwd().joinpath('malaria/cell_images/cell_images').is_dir():\n",
        "    pth = Path.cwd().joinpath('malaria/cell_images/cell_images')\n",
        "    shutil.rmtree(path=pth, ignore_errors=True)"
      ],
      "metadata": {
        "id": "A1QRP_AmBCpH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c6908e-cb6b-45ea-bfe4-b03785682ed2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to helper_modules/data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a helper_modules/data_loader.py\n",
        "\n",
        "# function to return all image paths as shuffled lists\n",
        "def _make_cells_list() -> tuple[list, list]:\n",
        "  # set random seed\n",
        "  random.seed(0)\n",
        "  # create paths of infected / uninfected\n",
        "  parasitized = Path.cwd().joinpath('malaria/cell_images/Parasitized')\n",
        "  uninfected = Path.cwd().joinpath('malaria/cell_images/Uninfected')\n",
        "\n",
        "  # lists of infected / parasitized paths\n",
        "  sick_list = list(parasitized.glob('*.png'))\n",
        "  healthy_list = list(uninfected.glob('*png'))\n",
        "\n",
        "  # all cells list\n",
        "  cell_list = sick_list + healthy_list\n",
        "\n",
        "  # sample random image paths from parasitized & uninfected directories\n",
        "  # 80% of random parasitized and unparasitized images will be sampled for training\n",
        "  # the rest will be split into validation and test datasets\n",
        "  train_list = chain(\n",
        "        sample(population=sick_list, k=int(len(sick_list)*0.80)),\n",
        "        sample(population=healthy_list, k=int(len(sick_list)*0.80)))\n",
        "\n",
        "  # and make a list of images to be used to training\n",
        "  train_list = list(train_list)\n",
        "\n",
        "  # delete the extracted image paths in train_list from cell_list\n",
        "  for i in train_list:\n",
        "    if i.is_file():\n",
        "      cell_list.remove(i)\n",
        "\n",
        "  # shuffle both lists\n",
        "  shuffle(cell_list), shuffle(train_list)\n",
        "\n",
        "  return cell_list, train_list\n"
      ],
      "metadata": {
        "id": "r48KVQ48HiPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "067cd8f6-be1b-4bcc-cfe8-6026e35e9a94"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to helper_modules/data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a helper_modules/data_loader.py\n",
        "\n",
        "# function to get dataloaders, and label-mapper dict\n",
        "def get_dataloaders() -> tuple[DataLoader, DataLoader, DataLoader, dict[int, str]]:\n",
        "  \"\"\"\n",
        "    Splits a dataset into training, validation, and test sets, and creates corresponding DataLoader\n",
        "    instances for each set. It also returns a dictionary that maps class labels to their respective\n",
        "    string labels.\n",
        "\n",
        "    The dataset is split into three subsets with the following proportions:\n",
        "    - 70% for training\n",
        "    - 20% for testing\n",
        "    - 10% for validation\n",
        "\n",
        "    The resulting DataLoaders are configured with a batch size of 32, use of all available CPU cores\n",
        "    for parallel data loading, and memory pinning for faster data transfer to the GPU.\n",
        "\n",
        "    Args:\n",
        "        None\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the following elements:\n",
        "            - train_dl (DataLoader): DataLoader for the training set.\n",
        "            - test_dl (DataLoader): DataLoader for the test set.\n",
        "            - val_dl (DataLoader): DataLoader for the validation set.\n",
        "            - class_mapper (dict): A dictionary that maps integer class labels to string labels,\n",
        "              where 0 is mapped to 'Uninfected' and 1 is mapped to 'Infected'.\n",
        "\n",
        "    Example:\n",
        "        train_dl, test_dl, val_dl, class_mapper = get_dataloaders()\n",
        "    \"\"\"\n",
        "  # download dataset\n",
        "  _download_dataset()\n",
        "\n",
        "  # get lists\n",
        "  cell_list, train_list = _make_cells_list()\n",
        "\n",
        "  # create a torch Dataset from training images\n",
        "  tr_set = CellsDataset(cells_list=train_list, transforms=_cell_transform)\n",
        "  # create a torch Dataset from the rest of images\n",
        "  val_ts_set = CellsDataset(cells_list=cell_list, transforms=_cell_transform)\n",
        "\n",
        "  # specify size of train, validation and test sets\n",
        "  val_size = int(0.75*len(cell_list))\n",
        "  ts_size = int(len(cell_list) - val_size)\n",
        "\n",
        "  # test and validation subsets\n",
        "  ts_set, val_set = random_split(\n",
        "      dataset=val_ts_set, lengths=[ts_size, val_size], generator=gen)\n",
        "\n",
        "  # create dataloaders for train, validation, test,\n",
        "  train_dl = DataLoader(\n",
        "      dataset=tr_set, batch_size=32, num_workers=os.cpu_count(), pin_memory=True, shuffle=True)\n",
        "  test_dl = DataLoader(\n",
        "      dataset=ts_set, batch_size=32, num_workers=os.cpu_count(), pin_memory=True, shuffle=False)\n",
        "  val_dl = DataLoader(\n",
        "      dataset=val_set, batch_size=32, num_workers=os.cpu_count(), pin_memory=True, shuffle=False)\n",
        "\n",
        "  # 0 -> Uninfected, 1 -> Infected\n",
        "  class_mapper = {0: 'Uninfected', 1:'Infected'}\n",
        "\n",
        "  return train_dl, test_dl, val_dl, class_mapper\n"
      ],
      "metadata": {
        "id": "vcA_qGUfBqQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de5cb7f7-7ca5-44bd-f8ed-2c59e746a113"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to helper_modules/data_loader.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the [`resnet`](https://www.digitalocean.com/community/tutorials/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet) architecture\n",
        "> With the help of the [`timm`](https://huggingface.co/docs/timm/v1.0.15/en/quickstart#quickstart) library, I'll load the [`resnet18`](https://huggingface.co/timm/resnet18.a1_in1k) CNN architecture and alter the `classifier` layer by specifying `2` classes\n",
        "\n",
        "> The function defined here will return a `model`, `Optimizer` and `loss function`\n",
        "+ Also, we'll use [`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#adam) as optimizer and [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#crossentropyloss) as loss function, (since the model will return _logits_ in the shape of `[*, 2]`)\n"
      ],
      "metadata": {
        "id": "-cevJd4pi3o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ”” **Info**\n",
        "+ The model will predict `0` for an `Uninfected` cell and `1` for an `Infected` cell\n",
        "+ For better generalization and to reduce training time, we'll leverage on **Transfer Learning.** Hence, the model we declare here will be **pre-trained** from the start\n",
        "+ Also, as earlier specified in `_cell_transform` above, the images will be normalized according to the normalization applied when pretraining the model. See how to do that from the `timm` quickstart guide linked [`here`](https://huggingface.co/docs/timm/v1.0.15/en/quickstart#image-augmentation)"
      ],
      "metadata": {
        "id": "fdKIa7SykM9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper_modules/model_builder.py\n",
        "import timm, torch\n",
        "\n",
        "# function to return model, optimizer and loss function\n",
        "def get_model(device:str) -> tuple[torch.nn.Module, torch.optim.Optimizer, torch.nn.Module]:\n",
        "  \"\"\"\n",
        "    Creates and initializes a model, optimizer, and loss function for training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    device : str\n",
        "        The device to which the model should be moved. Common values include 'cuda' for GPU or 'cpu' for CPU.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        A tuple containing three elements:\n",
        "        - model (torch.nn.Module): The model initialized with the resnet10t architecture for 2 classes.\n",
        "        - opt (torch.optim.Optimizer): The AdamW optimizer initialized with the model's parameters.\n",
        "        - loss_fn (torch.nn.Module): The CrossEntropyLoss function used for multi-class classification.\n",
        "    \"\"\"\n",
        "  # model\n",
        "  model = timm.create_model(\n",
        "      model_name='resnet18',\n",
        "      num_classes=2,\n",
        "      pretrained=True).to(device)\n",
        "\n",
        "  # optimizer\n",
        "  opt = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
        "\n",
        "  # loss function\n",
        "  loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "  return model, opt, loss_fn"
      ],
      "metadata": {
        "id": "7Sxd4ONdmElh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdbcdf02-feff-47d6-c233-94367341199e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing helper_modules/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early stopping\n",
        "> ðŸ’Ž **Pro Tip**\n",
        "\n",
        "> [Early stopping](https://www.linkedin.com/advice/1/what-benefits-drawbacks-early-stopping#:~:text=Early%20stopping%20is%20a%20form,to%20increase%20or%20stops%20improving.) is a mechanism of stopping training when the validation loss stops improving; with a view to preventing _overfitting_ on the training data\n",
        "+ Here, we'll create a class to take care of _early-stopping_"
      ],
      "metadata": {
        "id": "1O0iefHwwMkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper_modules/utils.py\n",
        "import torch, pathlib, numpy as np, matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from torch.utils.data import Dataset, Subset\n",
        "import torch\n",
        "from copy import deepcopy\n",
        "\n",
        "class EarlyStopping:\n",
        "  \"\"\"\n",
        "  Early stopping to prevent overfitting.\n",
        "\n",
        "  Attributes\n",
        "  ----------\n",
        "  counter : int\n",
        "      Counter to track the number of epochs without improvement.\n",
        "  patience : int\n",
        "      Number of epochs to wait after the last best score.\n",
        "  min_delta : float\n",
        "      Minimum change in the monitored quantity to qualify as an improvement.\n",
        "  score_type : str\n",
        "      'loss' or 'metric', determines the direction of improvement.\n",
        "  best_epoch : int\n",
        "      Epoch with the best score.\n",
        "  best_score : float\n",
        "      Best score achieved so far.\n",
        "  best_state_dict : dict\n",
        "      State dictionary of the model at the best score.\n",
        "  stop_early : bool\n",
        "      Flag to indicate if early stopping should be triggered.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, score_type: str, min_delta: float = 0.0, patience: int = 5):\n",
        "    \"\"\"\n",
        "    Initializes the EarlyStopping object.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    score_type : str\n",
        "        'loss' or 'metric', determines the direction of improvement.\n",
        "    min_delta : float, optional\n",
        "        Minimum change in the monitored quantity to qualify as an improvement. Defaults to 0.0.\n",
        "    patience : int, optional\n",
        "        Number of epochs to wait after the last best score. Defaults to 5.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    Exception\n",
        "        If score_type is not 'metric' or 'loss'.\n",
        "    \"\"\"\n",
        "    self.counter = 0\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.score_type = score_type\n",
        "    self.best_epoch = None\n",
        "    self.best_score = None\n",
        "    self.best_state_dict = None\n",
        "    self.stop_early = False\n",
        "\n",
        "    if (self.score_type != 'metric') and (self.score_type != 'loss'):\n",
        "        err_msg = 'score_type can only be \"metric\" or \"loss\"'\n",
        "        raise Exception(err_msg)\n",
        "\n",
        "  def __call__(self, model: torch.nn.Module, ep: int, ts_score: float):\n",
        "    \"\"\"\n",
        "    Checks if early stopping should be triggered based on the current score.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : torch.nn.Module\n",
        "        The model being trained.\n",
        "    ep : int\n",
        "        The current epoch number.\n",
        "    ts_score : float\n",
        "        The current score (loss or metric).\n",
        "    \"\"\"\n",
        "    if self.best_epoch is None:\n",
        "        self.best_epoch = ep\n",
        "        self.best_score = ts_score\n",
        "        self.best_state_dict = deepcopy(model.state_dict())\n",
        "\n",
        "    elif (self.best_score - ts_score >= self.min_delta) and (self.score_type == 'loss'):\n",
        "        self.best_epoch = ep\n",
        "        self.best_score = ts_score\n",
        "        self.best_state_dict = deepcopy(model.state_dict())\n",
        "        self.counter = 0\n",
        "\n",
        "    elif (ts_score - self.best_score >= self.min_delta) and (self.score_type == 'metric'):\n",
        "        self.best_epoch = ep\n",
        "        self.best_score = ts_score\n",
        "        self.best_state_dict = deepcopy(model.state_dict())\n",
        "        self.counter = 0\n",
        "\n",
        "    else:\n",
        "        self.counter += 1\n",
        "        if self.counter >= self.patience:\n",
        "            self.stop_early = True\n"
      ],
      "metadata": {
        "id": "kJfNgLoq8YL1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c37b7d3-723b-47c6-9286-dd01c04cd088"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing helper_modules/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training / evaluation\n",
        "> Here, I'll define functions for training and testing batches of data, as well as a function to return true labels, `y_true`, prediction labels, `y_pred` and prediction probabilities `y_proba`"
      ],
      "metadata": {
        "id": "JNLF2yZfxo-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile helper_modules/train_test.py\n",
        "import torch, numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import recall_score, accuracy_score\n",
        "\n",
        "# del torch, F, f1_score, accuracy_score\n",
        "\n",
        "# function for model training\n",
        "def train_batches(model: torch.nn.Module, train_dl: torch.utils.data.DataLoader,\n",
        "                  optimizer: torch.optim.Optimizer, loss_fn: torch.nn.Module, device: str) -> tuple[float, float, float]:\n",
        "  \"\"\"\n",
        "  Trains model on all batches of the training set DataLoader and returns\n",
        "  average training loss, accuracy, and F1 score.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : torch.nn.Module\n",
        "      The model being trained.\n",
        "  train_dl : torch.utils.data.DataLoader\n",
        "      DataLoader for training data.\n",
        "  optimizer : torch.optim.Optimizer\n",
        "      The optimizer.\n",
        "  loss_fn : torch.nn.Module\n",
        "      Function used to calculate loss.\n",
        "  device : str\n",
        "      The device on which computation occurs.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "      A tuple containing:\n",
        "          - ls (float): Average training loss across all batches.\n",
        "          - acc (float): Average training accuracy across all batches.\n",
        "          - rec (float): Average training recall score  across all batches.\n",
        "  \"\"\"\n",
        "  # for reproducibility\n",
        "  torch.manual_seed(0)\n",
        "  torch.cuda.manual_seed(0)\n",
        "  ls, acc, rec = 0, 0, 0\n",
        "\n",
        "  # training mode\n",
        "  model.train()\n",
        "\n",
        "  for x, y in train_dl:\n",
        "      # move x, y to device\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      # zero_grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward pass\n",
        "      logits = model(x)\n",
        "      y_pred = F.softmax(logits, dim=1).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "      # loss\n",
        "      loss = loss_fn(logits, y)\n",
        "      # accumulate values\n",
        "      ls += loss.item()\n",
        "      acc += accuracy_score(y_true=y.cpu().numpy(), y_pred=y_pred)\n",
        "      rec += recall_score(y_true=y.cpu().numpy(), y_pred=y_pred)\n",
        "\n",
        "      # back propagation\n",
        "      loss.backward()\n",
        "      # optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "  # compute averages\n",
        "  ls /= len(train_dl)\n",
        "  acc /= len(train_dl)\n",
        "  rec /= len(train_dl)\n",
        "\n",
        "  # return values\n",
        "  return ls, acc, rec\n",
        "\n",
        "\n",
        "def test_batches(model: torch.nn.Module, val_dl: torch.utils.data.DataLoader,\n",
        "                 loss_fn: torch.nn.Module, device: str) -> tuple[float, float, float]:\n",
        "  \"\"\"\n",
        "  Evaluates model on all batches of the test set DataLoader and returns\n",
        "  average test loss, accuracy, and F1 score.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : torch.nn.Module\n",
        "      The model being evaluated.\n",
        "  test_dl : torch.utils.data.DataLoader\n",
        "      DataLoader for test data.\n",
        "  loss_fn : torch.nn.Module\n",
        "      Function used to calculate loss.\n",
        "  device : str\n",
        "      The device on which computation occurs.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "      A tuple containing:\n",
        "          - ls (float): Average test loss across all batches.\n",
        "          - acc (float): Average test accuracy across all batches.\n",
        "          - rec (float): Average test recall score across all batches.\n",
        "  \"\"\"\n",
        "  ls, rec, acc = 0, 0, 0\n",
        "\n",
        "  # evaluation-mode\n",
        "  model.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    for x, y in val_dl:\n",
        "        # move x, y to device\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # forward pass\n",
        "        logits = model(x)\n",
        "        y_pred = F.softmax(logits, dim=1).argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        # loss\n",
        "        loss = loss_fn(logits, y)\n",
        "\n",
        "        # accumulate values\n",
        "        ls += loss.item()\n",
        "        acc += accuracy_score(y_true=y.cpu().numpy(), y_pred=y_pred)\n",
        "        rec += recall_score(y_true=y.cpu().numpy(), y_pred=y_pred)\n",
        "\n",
        "  # compute averages\n",
        "  ls /= len(val_dl)\n",
        "  acc /= len(val_dl)\n",
        "  rec /= len(val_dl)\n",
        "\n",
        "  # return values\n",
        "  return ls, acc, rec\n",
        "\n",
        "\n",
        "def true_preds_proba(model: torch.nn.Module, test_dl: torch.utils.data.DataLoader,\n",
        "                     device: str) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "  \"\"\"\n",
        "  A function that returns true labels, predictions, and prediction probabilities\n",
        "  from the passed DataLoader.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : torch.nn.Module\n",
        "      A neural network that subclasses torch.nn.Module.\n",
        "  test_dl : torch.utils.data.DataLoader\n",
        "      A DataLoader for the test dataset.\n",
        "  device : str\n",
        "      The device on which computation occurs.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "      A tuple containing:\n",
        "          - y_true (np.ndarray): A numpy array with true labels.\n",
        "          - y_pred (np.ndarray): A numpy array with predicted labels.\n",
        "          - y_proba (np.ndarray): A numpy array with predicted probabilities.\n",
        "  \"\"\"\n",
        "  # empty lists\n",
        "  y_true, y_preds, y_proba = list(), list(), list()\n",
        "  with torch.inference_mode():\n",
        "      model.eval()  # set eval mode\n",
        "      for x, y in test_dl:\n",
        "          # move x to device\n",
        "          x = x.to(device)\n",
        "\n",
        "          # make prediction\n",
        "          logits = model(x)\n",
        "\n",
        "          # prediction and probabilities\n",
        "          proba = F.softmax(logits, dim=1)\n",
        "          pred = F.softmax(logits, dim=1).argmax(dim=1)\n",
        "\n",
        "          # append\n",
        "          y_preds.append(pred)\n",
        "          y_proba.append(proba)\n",
        "          y_true.append(y)\n",
        "\n",
        "  y_preds = torch.concatenate(y_preds).cpu().numpy()\n",
        "  y_proba = torch.concatenate(y_proba).cpu().numpy()\n",
        "  y_true = torch.concatenate(y_true).numpy()\n",
        "\n",
        "  return y_true, y_preds, y_proba\n"
      ],
      "metadata": {
        "id": "ixNnOWrKxrci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df9c5612-89f5-4d0f-ac98-4a0e46544e4f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing helper_modules/train_test.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot results\n",
        "> Here, I'll define helper functions for plotting training metrics"
      ],
      "metadata": {
        "id": "GE2tc9RUahxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a helper_modules/utils.py\n",
        "\n",
        "# function to plot train and test results\n",
        "def plot_train_results(ep_list: list, train_score: list, test_score: list,\n",
        "                       ylabel: str, title: str, best_epoch: int):\n",
        "  \"\"\"\n",
        "  Plots training and test results against each other.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  ep_list : list\n",
        "      A list containing all epochs used in the optimization loop.\n",
        "  train_score : list\n",
        "      A list containing the training scores from the optimization loop.\n",
        "  test_score : list\n",
        "      A list containing the test scores from the optimization loop.\n",
        "  ylabel : str\n",
        "      Label for the y-axis of the plot.\n",
        "  title : str\n",
        "      Title for the plot.\n",
        "  best_epoch : int\n",
        "      Best epoch for which early stopping occurred.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  None\n",
        "  \"\"\"\n",
        "  f, ax = plt.subplots(figsize=(5, 3), layout='constrained')\n",
        "\n",
        "  # train loss\n",
        "  ax.plot(ep_list, train_score, label='Training',\n",
        "          linewidth=1.7, color='#0047ab')\n",
        "\n",
        "  # test loss\n",
        "  ax.plot(ep_list, test_score, label='Validation',\n",
        "          linewidth=1.7, color='#990000')\n",
        "  # vertical line (for early stopping)\n",
        "  if best_epoch is not None:\n",
        "      ax.axvline(best_epoch, linestyle='--', color='#000000', linewidth=1.0,\n",
        "                  label=f'Best ep ({best_epoch})')\n",
        "\n",
        "  # axis, title\n",
        "  ax.set_title(title, weight='black')\n",
        "  ax.set_ylabel(ylabel)\n",
        "  ax.set_xlabel('Epoch')\n",
        "  ax.tick_params(axis='both', labelsize=9)\n",
        "  plt.grid(color='#e5e4e2')\n",
        "\n",
        "  # legend\n",
        "  f.legend(fontsize=9, loc='upper right',\n",
        "            bbox_to_anchor=(1.28, 0.93),\n",
        "            fancybox=False)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "  \"\"\"\n",
        "  Plots a confusion matrix for all classes.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  y_true : np.ndarray\n",
        "      An ndarray containing the true label values.\n",
        "  y_pred : np.ndarray\n",
        "      An ndarray containing the predicted label values.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  None\n",
        "  \"\"\"\n",
        "  # define figure and plot\n",
        "  _, ax = plt.subplots(figsize=(3.0, 3.0), layout='compressed')\n",
        "  # plot\n",
        "  ConfusionMatrixDisplay.from_predictions(\n",
        "      y_true=y_true,\n",
        "      y_pred=y_pred, cmap='Blues', colorbar=False, ax=ax)\n",
        "\n",
        "  # set x and y labels\n",
        "  ax.set_ylabel('True Labels', weight='black')\n",
        "  ax.set_xlabel('Predicted Labels', weight='black',\n",
        "                color='#dc143c')\n",
        "  # set tick size and position\n",
        "  ax.xaxis.tick_top()\n",
        "  ax.xaxis.set_label_position('top')\n",
        "  ax.tick_params(axis='both', labelsize=9)\n",
        "\n",
        "  # change annotation font\n",
        "  for txt in ax.texts:\n",
        "      txt.set_fontsize(9)\n",
        "\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "UpswMomY6dB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1204e9d4-8c34-4a9e-8282-41255601956b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to helper_modules/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model\n",
        "> ðŸ”” **Info**\n",
        "\n",
        "> Pytorch's recommended way of saving a model is by saving its [`state_dict`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict). To do this, the [documentation](https://pytorch.org/tutorials/beginner/saving_loading_models.html#save-load-state-dict-recommended) recommends calling [`torch.save(obj=model.state_dict(), f=PATH)`](https://pytorch.org/docs/stable/generated/torch.save.html#torch-save)\n",
        "+ `f` - a file-like object or a string or `os.PathLike` object containing a file name. To work with paths, we'll use Python's [`pathlib`](https://docs.python.org/3/library/pathlib.html) module\n",
        "+ A common PyTorch convention is to save models using either a `.pt` or `.pth` file extension\n",
        "+ Also, it's good practice to move the model to the `cpu` before saving its `state_dict`"
      ],
      "metadata": {
        "id": "q8dmvDKZjJco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a helper_modules/utils.py\n",
        "\n",
        "# function to save model to specified directory\n",
        "def save_model(model: torch.nn.Module, path: pathlib.PosixPath):\n",
        "    \"\"\"\n",
        "    Saves the model's state_dict to a specified path.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : torch.nn.Module\n",
        "        The model to save.\n",
        "    path : pathlib.PosixPath\n",
        "        The path where the model's state_dict will be saved.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "    \"\"\"\n",
        "    torch.save(obj=model.cpu().state_dict(), f=path)\n",
        "    print(f\"MODEL'S state_dict SAVED TO: {path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzvslsO4nTJL",
        "outputId": "573da71d-ce46-44cc-e64c-0880c02efd90"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to helper_modules/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load saved model\n",
        "> To load a previously saved model's `state_dict`, we call\n",
        " [`torch.load(f=PATH, weights_only=True)`](https://pytorch.org/docs/stable/generated/torch.load.html#torch.load) that loads an object saved using [`torch.save()`](https://pytorch.org/docs/stable/generated/torch.save.html#torch-save) from a file:\n",
        "\n",
        "```\n",
        "    model = TheModelClass(*args, **kwargs)\n",
        "    model.load_state_dict(torch.load(PATH, weights_only=True))\n",
        "    model.eval()\n",
        "```\n",
        "\n",
        "> ðŸ”” **Info**\n",
        "+ Remember that you must call `model.eval()` before running inference\n",
        "+ `f` - a file-like object or a string or `os.PathLike` object containing a file name. To work with paths, we'll use Python's [`pathlib`](https://docs.python.org/3/library/pathlib.html) module\n",
        "+ Note that a `model` class must have been defined earlier, before calling [`model.load_state_dict()`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.load_state_dict) on the object"
      ],
      "metadata": {
        "id": "5f5k3J5rcAlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a helper_modules/utils.py\n",
        "\n",
        "# function to load model from a specified path\n",
        "def load_model(model: torch.nn.Module, path: pathlib.PosixPath):\n",
        "  \"\"\"\n",
        "  Loads the model's state_dict from a specified path.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : torch.nn.Module\n",
        "      A new object of the model class.\n",
        "  path : pathlib.PosixPath\n",
        "      Path pointing to a previously saved model's state_dict.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  model : torch.nn.Module\n",
        "      The model returned after loading the state_dict.\n",
        "  \"\"\"\n",
        "  # overwrite state_dict\n",
        "  model.load_state_dict(torch.load(f=path, weights_only=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLlas76CcGLJ",
        "outputId": "90c337ab-e808-4e23-b581-80d404855f8f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to helper_modules/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make inference\n",
        "> Here, I'll declare functions to make inference:\n",
        "+ On a single random image\n",
        "+ On multiple `[12]` random images"
      ],
      "metadata": {
        "id": "qiK8QJ57hASq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile -a helper_modules/utils.py\n",
        "\n",
        "# function to make inference on a single random image\n",
        "def make_single_inference(model: torch.nn.Module, dataset: torch.utils.data.Dataset,\n",
        "                          label_map: dict, device: str):\n",
        "  \"\"\"\n",
        "  Makes inference using a random data point from the test dataset.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : torch.nn.Module\n",
        "      A model (subclassing torch.nn.Module) to make inference.\n",
        "  dataset : torch.utils.data.Dataset\n",
        "      The Dataset to use for testing purposes.\n",
        "  label_map : dict\n",
        "      A dictionary mapping indices to labels (e.g., {0: 'O', 1: 'X'}).\n",
        "  device : str\n",
        "      Device on which to perform computation.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  None\n",
        "  \"\"\"\n",
        "  # get random image from test_set\n",
        "  idx = np.random.choice(len(dataset))\n",
        "  img, lb = dataset[idx]\n",
        "\n",
        "  # make prediction\n",
        "  with torch.inference_mode():\n",
        "    model.to(device)  # move model to device\n",
        "    model.eval()  # set eval mode\n",
        "    lgts = model.to(device)(img.unsqueeze(0).to(device))\n",
        "    pred = F.softmax(lgts, dim=1).argmax(dim=1)\n",
        "\n",
        "  # print actual retrieved image\n",
        "  plt.figure(figsize=(2.0, 2.0))\n",
        "  # title with label\n",
        "  if pred == lb:\n",
        "    plt.title(\n",
        "        f'Actual: {label_map[lb]}\\nPred: {label_map[pred.item()]}',\n",
        "        fontsize=7)\n",
        "  else:  # if labels do not match, title = with red color\n",
        "    plt.title(\n",
        "        f'Actual: {label_map[lb]}\\nPred: {label_map[pred.item()]}',\n",
        "        fontsize=7, color='#de3163', weight='black')\n",
        "  plt.axis(False)\n",
        "  plt.imshow(img.permute(1,2,0).clamp(min=0, max=1))\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def make_multiple_inference(model: torch.nn.Module, dataset: torch.utils.data.Dataset,\n",
        "                          label_map: dict, device: str):\n",
        "  \"\"\"\n",
        "  Makes inference on multiple random images from the test dataset.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : torch.nn.Module\n",
        "      A model (subclassing torch.nn.Module) to make inference.\n",
        "  dataset : torch.utils.data.Dataset\n",
        "      The Dataset used for evaluation purposes.\n",
        "  label_map : dict\n",
        "      A dictionary mapping indices to labels (e.g., {0: 'O', 1: 'X'}).\n",
        "  device : str\n",
        "      Device on which to perform computation.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  None\n",
        "  \"\"\"\n",
        "  # get array of 12 random indices of images in test_dataset\n",
        "  indices = np.random.choice(len(dataset), size=12, replace=False)\n",
        "  # create subset from the 12 indices\n",
        "  sub_set = Subset(dataset=dataset, indices=indices)\n",
        "\n",
        "  # define a figure and subplots\n",
        "  f, axs = plt.subplots(2, 6, figsize=(7.5, 5.5), layout='compressed')\n",
        "\n",
        "  # move model to device & set eval mode\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "\n",
        "  # loop through each subplot\n",
        "  for i, ax in enumerate(axs.flat):\n",
        "    img, lb = sub_set[i]  # return image and label\n",
        "\n",
        "    # make inference on image returned\n",
        "    with torch.inference_mode():\n",
        "        lg = model(img.unsqueeze(0).to(device))\n",
        "        pred = F.softmax(lg, dim=1).argmax(dim=1)\n",
        "\n",
        "    ax.imshow(img.permute(1,2,0).clamp(min=0, max=1))\n",
        "    ax.axis(False)\n",
        "    if pred == lb:\n",
        "        ax.set_title(\n",
        "            f'Actual: {label_map[lb]}\\nPred: {label_map[pred.item()]}',\n",
        "            fontsize=7)\n",
        "    else:  # if labels do not match, title = with red color\n",
        "        ax.set_title(\n",
        "            f'Actual: {label_map[lb]}\\nPred: {label_map[pred.item()]}',\n",
        "            fontsize=7, color='#de3163', weight='black')\n",
        "\n",
        "  f.suptitle('Inference Made on 12 Random Test Images',\n",
        "              weight='black', y=0.83)\n",
        "  plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ0t1UlRhDHP",
        "outputId": "66018972-9cff-4b79-fb37-c661101d5dc5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appending to helper_modules/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Archive modules\n",
        "> Here, we'll create a function to archive all the modules `*.py` files into a `*.zip` file with the help of [`make_archive`](https://docs.python.org/3/library/shutil.html#shutil.make_archive) function from the [`shutil`](https://docs.python.org/3/library/shutil.html#module-shutil) python module\n",
        "\n",
        "> âœ‹ **Info**\n",
        "+ The `zip` file containing the helper modules will be then uploaded to the GitHub repository [here](https://github.com/Martinmbiro/Malaria/raw/refs/heads/main/helper%20modules/modules.zip). That way, the modules can be downloaded and extracted dynamically in code"
      ],
      "metadata": {
        "id": "KyrG_C_hni52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, pathlib\n",
        "\n",
        "def archive_modules(path_to_files: pathlib.PosixPath, zip_name: str):\n",
        "    \"\"\"\n",
        "    Archive a directory into a ZIP file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path_to_files : pathlib.PosixPath\n",
        "        The path to the directory or files to be archived.\n",
        "\n",
        "    zip_name : str\n",
        "        The name of the resulting ZIP file (without the extension).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "        This function does not return any value. It creates a ZIP archive at the specified location.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    This function uses `shutil.make_archive` to create the archive. The archive will be created\n",
        "    in the current working directory unless a full path is provided in the `zip_name`.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> archive_modules(pathlib.Path('/path/to/files'), 'my_archive')\n",
        "    This will create a ZIP archive named 'my_archive.zip' containing the files from the specified directory.\n",
        "\n",
        "    \"\"\"\n",
        "    shutil.make_archive(\n",
        "        base_name=zip_name, format='zip', root_dir=path_to_files)"
      ],
      "metadata": {
        "id": "xjfGfN7BZ9CK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# archive modules\n",
        "archive_modules(HELPER_MODULES, 'modules')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGGwUg_ebUDI",
        "outputId": "4b35a5bd-6293-4ee2-a90b-3963c10e7d01"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.47 ms, sys: 1.92 ms, total: 3.39 ms\n",
            "Wall time: 6.9 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> â–¶ï¸ **Up Next**\n",
        "+ Having created modules out of the most reusable code, I'll implement an end to tend project for classifying playing card images in the subsequent notebook, `02 Malaria end to end.ipynb`"
      ],
      "metadata": {
        "id": "huE7PJuzGBva"
      }
    }
  ]
}